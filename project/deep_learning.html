<!DOCTYPE html>

<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <link rel="stylesheet" href="project_style.css" />
        <link
        rel="stylesheet"
        href="https://fonts.googleapis.com/css?family=Quicksand"
        />
        <link
        rel="stylesheet"
        href="https://fonts.googleapis.com/css?family=Assistant"
        />
        <link
        rel="stylesheet"
        href="https://fonts.googleapis.com/css?family=Abel"
        />
        <link
        rel="stylesheet"
        href="https://fonts.googleapis.com/css?family=Open+Sans"
        />
        <link
        rel="stylesheet"
        href="https://fonts.googleapis.com/icon?family=Material+Icons"
        />
        <title>Portfolio - Lok Tze Lun</title>
        <base href="https://tzelun.github.io/LokTzeLun-Portfolio/">
    </head>
    <body>
        <header id="header">
            <img id="logo" src="home/mylogo.PNG" />
            <nav id="nav-bar">
                <a href="index.html" class="nav-link">Home</a>
                <a href="docs/education.html" class="nav-link">Education</a>
                <a href="docs/project.html" class="nav-link">Projects</a>
                <a href="docs/work.html" class="nav-link">Work</a>
                <div class="dropdown">
                    <button id="menu-btn" class="menu-btn" onclick="dropdown()">
                        <i class="material-icons">menu</i>
                    </button>
                    <div id="dropdown-content" class="dropdown-content">
                        <a href="index.html" class="drop-link">Home</a>
                        <a href="docs/education.html" class="drop-link">Education</a>
                        <a href="docs/project.html" class="drop-link">Projects</a>
                        <a href="docs/work.html" class="drop-link">Work</a>
                    </div>
                </div>
            </nav>
        </header>

        <div class="back">
            <a href="docs/project.html">
                <i class="material-icons">arrow_back</i> Projects
            </a>
        </div>

        <div class="container">
            <h1 id="title">
                Deep Learning
            </h1>
    
            <div id="table-of-contents">
                <h2>Contents:</h2>
                <ol>
                    <li>
                        <div onclick="bookmark('p2')">
                            A Real-time Realistic Scenario-based Bone Drilling Audio Generator
                        </div>
                    </li>
                    <li>
                        <div onclick="bookmark('p1')">
                            Single Image Super Resolution (SISR) with Deep Back-ProjectiNetwork (DBPN)
                        </div>
                    </li>
                </ol>
            </div>

            <div class="contents">
                <div class="items">
                    <h1 id="p2">
                        A Real-time Realistic Scenario-based Bone Drilling Audio Generator
                    </h1>
                    <p>
                        This is the Deep Audio Generator used in tandem with the bone drilling simulator as
                        mentioned in the Creative Informatics project section. This audio generator could generate audio
                        at a speed of ~23Hz with CPU and 156Hz with GPU (NVIDIA TITAN RTX). Besides, it could
                        generate realistic audio visually (as mel spectrogram) and audibly.
                    </p>
            
                    <div class="keywords">
                      <div>Audio processing</div>
                      <div>Mel spectrogram</div>
                      <div>Generative adversarial network (GAN)</div>
                      <div>CGAN</div>
                      <div>WGAN-GP</div>
                      <div>DCGAN</div>
                      <div>Audio generation</div>
                      <div>HiFi-GAN</div>
                      <div>Latent vector estimation</div>
                      <div>Latent vector interpolation</div>
                      <div>PyTorch</div>
                      <div>Python</div>
                      <div>Frechet inception distance</div>
                      <div>Frechet audio distance</div>
                    </div>

                    <p>
                        The generator must generate audio given the drilling scenario. In other words, depending
                        how the drill operates onto the structure, the drilling audio is perceivably different.
                        In my research, drilling angle, drilling force, and surface dryness (irrigation is a standard
                        procedure in invasive surgery) were the factors considered in affecting the sound. The drill angle
                        and force are used as conditional inputs, whereas the surface dryness influences the latent space
                        used by GAN-based networks to generate an output.
                    </p>
                    
                    <p>
                        <strong>Architecture of Deep Audio Generator (DAG):</strong>
                    </p>
                    <div class="gallery">
                        <img id="lg_img" src="project/assets/dl/p2_1.png">
                    </div>
                    <p>
                        DAG is a two-stage system comprising of two independent deep learning model.
                        In the first stage, mel spectrogram tensor is generated using a conditional
                        DCGAN trained with Wasserstein loss and gradient penalty, called C-WGAN-GP.
                        This spectrogram intermediary is inverted using the High Fidelity GAN (HiFi-GAN).
                        Both models are trained with my own bespoke drilling audio dataset.
                        As C-WGAN-GP was developed by myself, kindly check out the repository at
                        <a href="https://github.com/TzeLun/DeepMelSpectrogramGenerator" target="_blank">
                            [GitHub] <i class="material-icons">open_in_new</i>
                        </a> if interested.
                    </p>
                    <p>
                        <strong>Method to alter the generated audio with surface dryness:</strong>
                    </p>
                    <div class="gallery">
                        <img id="lg_img" src="project/assets/dl/p2_2.png">
                    </div>
                    <p>
                        <strong>Summary of research work for deep learning section (red box):</strong>
                    </p>
                    <div class="gallery">
                        <img id="lg_img" src="project/assets/dl/p2_5.png">
                    </div>
                    <p>
                        <strong>
                            Results
                            (Comparing the mel spectrogram of generated audio with the original counterpart):</strong>
                    </p>                  
                    <div class="gallery">
                        <img id="lg_img" src="project/assets/dl/p2_4.png">
                    </div>
                    <p>
                        <strong>
                            Results
                            (Generating audio in between wet and dry surfaces
                            with latent space interpolation):</strong>
                            <br>
                            Wet surfaces produce audio with low frequency bands (observe the top of the mel spectrogram)
                    </p>                  
                    <div class="gallery">
                        <img id="lg_img" src="project/assets/dl/p2_3.png">
                    </div>
                    <p>
                        <strong>
                            Results
                            (Comparison of real and generated audio of the same conditions):</strong>
                            <br>
                            Tune up your device volume if the audio is too faint
                    </p>
                    <div class="audio-gallery">
                        <div>
                            <div>
                                Real:
                            </div>
                            <div>
                                <audio src="project/assets/dl/real_sample.wav" controls>
                                </audio>
                            </div>
                        </div>
                        <div>
                            <div>
                                Fake:
                            </div>
                            <div>
                                <audio src="project/assets/dl/fake_sample.wav" controls>
                                </audio>
                            </div>
                        </div>
                    </div>
                    <p>
                        <strong>Description of research work:</strong>
                    </p>
                    <ul>
                      <li>
                        Developed a bespoke data acquisition rig consisting of force sensor and microphone
                        to record data from drilling experiments.
                      </li><br>
                      <li>
                        Devised the audio dataset. Work includes: labelling audio data with drill force and angle,
                        conversion of audio waveform into mel spectrogram with TorchAudio and Librosa, offline data
                        augmentation using time transposition on audio recordings.
                      </li><br>
                      <li>
                        Developed C-WGAN-GP from scratch in Python with PyTorch, including the training and inferencing
                        script. Forked and modified the HiFi-GAN repository to suit the project requirements.
                        Trained both generators with a single NVIDIA TITAN RTX for GPU acceleration .
                      </li><br>
                      <li>
                        Proposed and developed a Monte Carlo method to estimate the wet and dry latent vectors without
                        training an encoder model. This method incorporates an audio generation evaluation metric - Frechet
                        Audio Distance (FAD) to classify latent vectors to their surface conditions.
                      </li><br>
                      <li>
                        Generated drilling audio conditionally with the influence of surface dryness through interpolating
                        between the wet and dry latent vectors. The dryness factor is as an adjustable
                        parameter when piloting the simulator.
                      </li><br>
                      <li>
                        Compared the fidelity of DAG with previous simulator audio scheme, pitch modulation using
                        Frechet Inception Distance (FID) for mel spectrogram and Frechet Audio Distance (FAD) for
                        audio waveforms. Results evidently proved the effectiveness of DAG in generating high
                        fidelity audio.
                      </li><br>
                      <li>
                        Integrated the audio generator with the VR simulator through a ROS-based communication
                        protocol. Developed an audio streaming function with the Python Sounddevice library to
                        render audible generated audio after receiving the generating conditions from the VR simulator.
                      </li>
                    </ul>
                  </div>
                  <div class="items">
                    <h1 id="p1">
                        Single Image Super Resolution (SISR) with Deep Back-ProjectiNetwork (DBPN)
                    </h1>
                    <p>
                        Improve low resolution (LR) images to high resolution (HR) ones with deep learning. The main project is
                        in implementing DBPN in TensorFlow/Keras. Available models for study are
                        DBPN-M (lightweight) with/without error feedback (EF) and D-DBPN (more parameters & layers).
                        Check out the repository at 
                        <a href="https://github.com/TzeLun/DBPN-Tensorflow" target="_blank">
                            [GitHub] <i class="material-icons">open_in_new</i>
                        </a>
                    </p>
                    <div class="keywords">
                        <div>Computer vision</div>
                        <div>Image processing</div>
                        <div>Convolutional neural network</div>
                        <div>Python</div>
                        <div>TensorFlow/Keras</div>
                        <div>Back-ProjectiNetwork</div>
                        <div>Single image super resolution</div>
                      </div>
                    <p>
                        <strong>Architecture of DBPN:</strong>
                    </p>
                    <div class="gallery">
                        <img id="lg_img" src="project/assets/dl/p1_1.png">
                    </div>
                    <p>
                        <strong>Results after training with limited DIV2K dataset:</strong>
                    </p>
                    <div class="gallery">
                        <img id="lg_img"
                        src="project/assets/dl/p1_2.png">
                    </div>
            
                    <ul>
                      <li>
                        Developed the DBPN network from scratch in Python via TensorFlow and Keras
                        based on the original research paper.
                      </li><br>
                      <li>
                        Trained and tested three different configurations of DBPN on a DIV2K dataset.
                        Proven the importance of error feedback in the up/down-projection blocks.
                      </li><br>
                      <li>
                        Analysed and documented the architecture, concepts, and merits of DBPN into a
                        comprehensive summary report. (Check the github link above for the report)
                      </li>
                    </ul>
                  </div>
            </div>
        </div>

        <div id="go_top" onclick="bookmark('nav-bar')">TOP</div>

        <footer>&copy 2023 Lok Tze Lun - Portfolio All Rights Reserved.</footer>

        <script>
            window.onscroll = function() {scrollFunction()};

            function scrollFunction() {
                // if (document.body.scrollTop > 100 || document.documentElement.scrollTop > 100) {
                //     go_top.style.display = "block";
                // } else {
                //     go_top.style.display = "none";
                // }
                let go_top = document.getElementById("go_top");
                let table = document.getElementById("table-of-contents");
                let header_bar = document.getElementById("header");
                if (window.scrollY > (table.offsetTop + table.offsetHeight - header_bar.offsetHeight)) {
                    go_top.style.display = "block";
                } else {
                    go_top.style.display = "none";
                }
            }

            function dropdown() {
                var drop_content = document.getElementById("dropdown-content");
                var btn = document.getElementById("menu-btn");
                btn.classList.toggle("btn-select");
                if (drop_content.classList.contains("show")) {
                drop_content.classList.remove("show");
                } else {
                drop_content.classList.add("show");
                }
            }

            function bookmark(id) {
                let element = document.getElementById(id);
                let header_bar = document.getElementById("header");
                window.scrollTo(0, element.offsetTop - header_bar.offsetHeight);
            }
        </script>
    </body>
</html>
